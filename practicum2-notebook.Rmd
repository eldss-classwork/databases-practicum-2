---
title: "CS5200 Fall 2020: Practicum 2"
author: "Chandra Davis, Evan Douglass"
output:
  pdf_document: default
---

## Overview

We've decided to work with SQLite for this practicum. As such, to work with these files you will need SQLite installed on your machine. The data we are using is provided by IMDB at https://datasets.imdbws.com/

The code below will download all the compressed tsv files required by this notebook into a folder called `data/`. You can leave them in their compressed format, as that is how we will read them in to this notebook. The downloads can take some time, so if you already have the files in a `data/` folder, we will avoid downloading them again. If you would like to replace the files, for example if new ones were published, you can change the `updateData` variable in the next code block to `TRUE`.

## Setup

The first thing we need to do is download and process the data files we need for the practicum. These will be downloaded from IMDB and processed into smaller sample files for ease of working with in the rest of the notebook. We have used Python for this task because it has more built in language constructs for file and string processing without loading everything into memory as a dataframe. The remainder of the notebook will be R based.

```{r, include=FALSE}
# include option means no output

# False by default. Change to true if you need these files or want new versions.
# This may necessitate filtering the datasets as well.
# Caution: This will take a long time!
download <- True
process <- False

directory <- "./data/"
nameBasics <- "name.basics"
titleBasics <- "title.basics"
titleAkas <- "title.akas"
titleCrew <- "title.crew"
titleEpisode <- "title.episode"
titlePrincipals <- "title.principals"
titleRatings <- "title.ratings"
gz <- ".tsv.gz"
sample_tsv <- ".sample.tsv"

download_files <- function():
  #' Downloads all needed files from IMDB. Will overwrite existing files.
  url <- "https://datasets.imdbws.com/"
  files <- c(nameBasics, titleBasics, titleAkas, titleCrew, titleEpisode, 
           titlePrincipals, titleRatings)
  
  for file in files:
    urllib.request.urlretrieve(url, directory + file + gz)

def process_episodes(num_rows):
  '''
  To ensure titles will include episodes, we process the episodes file differently
  than the others. A set of episodes from the start of the file will define
  most of the titles included in the final sample dataset.
  '''
  print('hello')
#   original_path = directory + titleEpisode + gz
#   new_path = directory + titleEpisode + sample_tsv
#   with original as gzip.open(original_path, 'rt', encoding="utf-8"):
#     with new as open(new_path, 'w', encoding="utf-8"):
#       # Transfer headerline
#       line = original.readline()
#       new.write(line)
#       
#       # Transfer set number of lines
#       for _ in range(num_rows):
#         line = original.readline()
#         new.write(line)

if download:
  # Get the files
  download_files()
  
if process:
  title_ids = set()
  name_ids = set()
  num_rows_episodes = 500
  num_rows_names = 50
  
  # Process episodes so we can get title id set
  process_episodes(num_rows_episodes)
```


Before working with the data we need to create a place to store it. This section will set up a SQLite database and any constants needed later.

```{r, include=FALSE}

library(RSQLite)
library(readr)
library(tidyr)
library(dplyr)
library(data.table)
library(hash)

# Setup SQLite database
DB_NAME <- "imdb-data.db"
conn <- dbConnect(RSQLite::SQLite(), DB_NAME)

# Set up data folder and variables needed for download of datasets
dir.create("data", showWarnings=FALSE)

URL <- "https://datasets.imdbws.com/"
updateData <- FALSE  # Change to TRUE if you want to update these files
directory <- "./data/"
nameBasics <- "name.basics.tsv.gz"
titleBasics <- "title.basics.tsv.gz"
titleAkas <- "title.akas.tsv.gz"
titleCrew <- "title.crew.tsv.gz"
titleEpisode <- "title.episode.tsv.gz"
titlePrincipals <- "title.principals.tsv.gz"
titleRatings <- "title.ratings.tsv.gz"
```

The next section downloads all the data files needed, if they don't exist already. This may take a while if you do not already have the files.

```{r}
downloadData <- function(file) {
  #' Downloads a gziped file from imdb
  destination <- paste(directory,file,sep="")
  if(!file.exists(destination) || updateData){
    website <- paste(URL,file,sep="")
    download.file(website,destination)
  }
  destination # final statement in function returned
}

# Download data
# NOTE: only run this chunk once per execution of notebook, otherwise 
# filepaths will get messed up.
titleAkas <- downloadData(titleAkas)
titleCrew <- downloadData(titleCrew)
titleEpisode <- downloadData(titleEpisode)
nameBasics <- downloadData(nameBasics)
titlePrincipals <- downloadData(titlePrincipals)
titleRatings <- downloadData(titleRatings)
titleBasics <- downloadData(titleBasics)
```

We will also need the following functions to read in data from files and convert them to dataframes.

```{r}
createDataframe <- function(file, rows=-1) {
  #' Creates a dataframe out of a gzipped tsv file. Can specify a row limit
  #' with rows, but defaults to the whole file.
  fread(file, na.strings = "\\N", encoding = "UTF-8", data.table = FALSE, 
        showProgress = FALSE, nrows = rows)  # fread is a faster alternative to read.tsv
}

findMaxRowExactKey <- function(file, stopKeyValue) {
  #' Finds the row value in the gzipped tsv immediately before the given key
  #' value. stopKeyValue is assumed to be the first entry in a line of the tsv.
  #' It should also be the key above the last one you want to include, as there
  #' are some files with multiple of the same key in a row.
  fconn <- gzfile(file, 'r')
  rowNum <- -1
  while (TRUE) {
    # Read one line
    line <- readLines(fconn, n=1)
    if (length(line) == 0) {
      break
    }
    
    # Stop if the key is found, or on EOF
    foundLine <- startsWith(line, stopKeyValue)
    if (foundLine) {
      break
    }
    rowNum <- rowNum + 1
  }
  close(fconn)
  rowNum
}

findMaxRowLessThanKey <- function(file, stopKeyValue) {
  #' Similar to findMaxRowExactKey except it can be used in files where the 
  #' exact key may not be present. Will return the row value directly below
  #' the first row with a key greater than or equal to the given key. This
  #' function is less efficient than the exact key version because it requires
  #' parsing the number part of a key.
  fconn <- gzfile(file, 'r')
  rowNum <- 0
  stopKeyNum <- parse_number(stopKeyValue)
  while (TRUE) {
    # Read one line
    line <- readLines(fconn, n=1)
    
    # Ignore header line
    if (startsWith(line, "tconst")) {
      next
    }
    
    # Split line and parse number part of key
    parts <- strsplit(line, "\t")[[1]]
    keyNum <- parse_number(parts[1])
    
    # Stop if the key is found, or on EOF
    if (length(line) == 0 || keyNum >= stopKeyNum) {
      break
    }
    rowNum <- rowNum + 1
  }
  close(fconn)
  rowNum
}
```

## Database Schema

The following chunk creates all the necessary tables in our database based on the schema presented earlier.

```{r}
# First we need to remove any existing data,
# for example, if this has been run before.
drop_table <- function(table_name) {
  paste("DROP TABLE IF EXISTS ", table_name, ";", sep="")
}

# Saving rows affected to a var prevents output
rows_aff <- dbExecute(conn, "PRAGMA foreign_keys = OFF;") # Avoid FK checks
curr_tables <- dbListTables(conn)
for (table in curr_tables) {
  dbExecute(conn, drop_table(table))
}
rows_aff <- dbExecute(conn, "PRAGMA foreign_keys = ON;")

# Now we can create the tables
build_table <- function(table_def) {
  CREATE <- "CREATE TABLE IF NOT EXISTS"
  paste(CREATE, table_def)
}

# Build table definition list
tables <- c(
  build_table(
    "Title_Type (
      format_id INTEGER PRIMARY KEY,
      format TEXT NOT NULL
    );"
  ),
  build_table(
    "Media (
      tconst TEXT PRIMARY KEY,
      format_id INTEGER NOT NULL,
      primaryTitle TEXT,
      originalTitle TEXT,
      isAdult INTEGER, -- 0=false, else true
      startYear INTEGER,
      endYear INTEGER,
      runtimeMins INTEGER,
      FOREIGN KEY (format_id) REFERENCES Title_Type(format_id)
    );"
  ),
  build_table(
    "Ratings (
      tconst TEXT PRIMARY KEY,
      averageRating REAL
      numVotes INTEGER,
      FOREIGN KEY (tconst) REFERENCES Media(tconst)
    );"
  ),
  build_table(
    "Episode (
      tconst TEXT PRIMARY KEY,
      parentTconst TEXT NOT NULL,
      seasonNumer INTEGER,
      episodeNumber INTEGER,
      FOREIGN KEY (tconst) REFERENCES Media(tconst),
      FOREIGN KEY (parentTconst) REFERENCES Media(tconst)
    );"
  ),
  build_table(
    "Genres (
      genre_id INTEGER PRIMARY KEY,
      genre TEXT NOT NULL
    );"
  ),
  build_table(
    "Media_Genres (
      mg_id INTEGER PRIMARY KEY,
      tconst TEXT NOT NULL,
      genre_id INTEGER NOT NULL,
      FOREIGN KEY (tconst) REFERENCES Media(tconst),
      FOREIGN KEY (genre_id) REFERENCES Genres(genre_id),
      CONSTRAINT unique_mg_tc_gid UNIQUE (tconst, genre_id)
    );"
  ),
  build_table(
    "People (
      nconst TEXT PRIMARY KEY,
      primaryName TEXT,
      birthYear INTEGER,
      deathYear INTEGER,
      age INTEGER,
      numMovies INTEGER
    );"
  ),
  build_table(
    "Known_For_Titles (
      kt_id INTEGER PRIMARY KEY,
      nconst TEXT NOT NULL,
      tconst TEXT NOT NULL,
      FOREIGN KEY (tconst) REFERENCES Media(tconst),
      FOREIGN KEY (nconst) REFERENCES People(nconst),
      CONSTRAINT unique_kft_nc_tc UNIQUE (nconst, tconst)
    );"
  ),
  build_table(
    "Professions (
      prof_id INTEGER PRIMARY KEY,
      prof_title TEXT NOT NULL
    );"
  ),
  build_table(
    "Primary_Profession (
      pp_id INTEGER PRIMARY KEY,
      nconst TEXT NOT NULL,
      prof_id INTEGER NOT NULL,
      FOREIGN KEY (nconst) REFERENCES People(nconst),
      FOREIGN KEY (prof_id) REFERENCES Professions(prof_id),
      CONSTRAINT unique_pp_nc_pid UNIQUE (nconst, prof_id)
    );"
  ),
  build_table(
    "Crew (
      tconst TEXT NOT NULL,
      ordering INTEGER NOT NULL,
      nconst TEXT NOT NULL,
      category TEXT,
      job TEXT,
      characters TEXT,
      PRIMARY KEY (tconst, ordering),
      FOREIGN KEY (tconst) REFERENCES Media(tconst),
      FOREIGN KEY (nconst) REFERENCES People(nconst)
    );"
  ),
  build_table(
    "Media_Directors (
      md_id INTEGER PRIMARY KEY,
      tconst TEXT NOT NULL,
      nconst TEXT NOT NULL,
      FOREIGN KEY (tconst) REFERENCES Media(tconst),
      FOREIGN KEY (nconst) REFERENCES People(nconst),
      CONSTRAINT unique_md_nc_tc UNIQUE (nconst, tconst)
    );"
  ),
  build_table(
    "Media_Writers (
      mw_id INTEGER PRIMARY KEY,
      tconst TEXT NOT NULL,
      nconst TEXT NOT NULL,
      FOREIGN KEY (tconst) REFERENCES Media(tconst),
      FOREIGN KEY (nconst) REFERENCES People(nconst),
      CONSTRAINT unique_mw_nc_tc UNIQUE (nconst, tconst)
    );"
  ),
  build_table(
    "Also_Known_As (
      tconst TEXT NOT NULL,
      ordering INTEGER NOT NULL,
      title TEXT,
      region TEXT,
      language TEXT,
      isOriginalTitle INTEGER, -- 0=false, else true
      PRIMARY KEY (tconst, ordering),
      FOREIGN KEY (tconst) REFERENCES Media(tconst)
    );"
  ),
  build_table(
    "Title_Types (
      type_id INTEGER PRIMARY KEY,
      type TEXT NOT NULL
    );"
  ),
  build_table(
    "Aka_Types (
      akt_id INTEGER PRIMARY KEY,
      tconst TEXT NOT NULL,
      ordering INTEGER NOT NULL,
      type_id INTEGER NOT NULL,
      -- Media table PK tconst has ref. integrity through AKA Table
      FOREIGN KEY (tconst, ordering) REFERENCES Also_Known_As(tconst, ordering),
      FOREIGN KEY (type_id) REFERENCES Title_Types(type_id),
      CONSTRAINT unique_akat_tc_ord_tid UNIQUE (tconst, ordering, type_id)
    );"
  ),
  build_table(
    "Attributes (
      att_id INTEGER PRIMARY KEY,
      att_name TEXT NOT NULL
    );"
  ),
  build_table(
    "Aka_Attributes (
      aka_id INTEGER PRIMARY KEY,
      tconst TEXT NOT NULL,
      ordering INTEGER NOT NULL,
      att_id INTEGER NOT NULL,
      -- Media table PK tconst has ref. integrity through AKA Table
      FOREIGN KEY (tconst, ordering) REFERENCES Also_Known_As(tconst, ordering),
      FOREIGN KEY (att_id) REFERENCES Attributes(att_id),
      CONSTRAINT unique_akaatt_tc_ord_aid UNIQUE (tconst, ordering, att_id)
    );"
  )
)

# Actually create the tables
for (table_stmt in tables) {
  dbExecute(conn, table_stmt)
}
```

## Reading Data Locally

Next we need to load data into the database. Because of the size of the data set, we will take a small sample of it for the purposes of this practicum.

### Titles

The number of titles we choose to include will determine the amount of data read in from other files. Therefore we will get this data first.

```{r}
# We've decided arbitrarily to load 2000(?) titles/media
stopTconst <- "tt0002001"
rows <- findMaxRowExactKey(titleBasics, stopTconst)
df.tb <- createDataframe(titleBasics, rows)
tail(df.tb)
```

### AKAs

```{r}
rows <- findMaxRowExactKey(titleAkas, stopTconst)
df.akas <- createDataframe(titleAkas, rows)
tail(df.akas)
```

### Episodes

There are no episodes below tt0002000. Is this OK? How should we deal with this?

### Ratings

```{r}
rows <- findMaxRowLessThanKey(titleRatings, stopTconst)
df.ratings <- createDataframe(titleRatings, rows)
tail(df.ratings)
```

### Principles

Principles will help us determine which people to include

```{r}
rows <- findMaxRowExactKey(titlePrincipals, stopTconst)
df.princ <- createDataframe(titlePrincipals, rows)
tail(df.princ)
```

### Crew

Crew data also contains data about people who worked on titles/media.

```{r}
rows <- findMaxRowExactKey(titleCrew, stopTconst)
df.crew <- createDataframe(titleCrew, rows)
tail(df.crew)
```

The Crew data has some values in directors and writers that are separated by commas. Note the 7th row in the output below. These will have to be delt with in order to figure out which people we need to include in the database.

```{r}
head(df.crew, 10)
```

We use tidyverse packages to split out the writers and directors into separate dataframes, with each name id on it's own line.

```{r}
# Separate directors
df.directors <- df.crew[c("tconst", "directors")] %>%
  mutate(directors = strsplit(as.character(directors), ",")) %>%
  unnest(directors)

# Remove titles with no director listed
df.directors <- drop_na(df.directors)

head(df.directors, 10)
```
```{r}
# Separate writers
df.writers <- df.crew[c("tconst", "writers")] %>%
  mutate(writers = strsplit(as.character(writers), ",")) %>%
  unnest(writers)

# Remove rows with no writers listed
df.writers <- drop_na(df.writers)

head(df.writers, 10)
```

### Names/People

Getting data from the name basics data is more complicated. Unlike the other files, there is no natural ordering on the media IDs. However, we do have name IDs for every crew member on the media that we are including. By creating a set of these name IDs we can use it to filter out people we don't want.

Unfortunately we will have to read in the whole file, so this will take some time if running it yourself. We could filter the original file and write out a new one with the filtered rows, but this requires parsing the whole file anyways, so it makes more sense to simply read in the whole file to a dataframe and then filter that.

```{r}
# This may take a while...
df.names <- createDataframe(nameBasics)
tail(df.names)

```

```{r}
# Gather all unique name IDs into single series
allNames <- unique(
  union_all(
    as.character(df.princ$nconst),
    df.directors$directors,
    df.writers$writers
  )
)

# Add them to a hash
nameSet = hash(allNames, 1:length(allNames))  # values don't matter for the set

# Filter for used names
# This may take several minutes. May be longer than the reading...
df.names <- filter(df.names, has.key(nconst, nameSet))
tail(df.names)
```

## Loading Data to Database

