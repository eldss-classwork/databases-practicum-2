---
title: 'CS5200 Fall 2020: Practicum 2'
author: "Chandra Davis, Evan Douglass"
output:
  pdf_document: default
  html_document:
    df_print: paged
  word_document: default
---

## Overview

We've decided to work with SQLite for this practicum. As such, to work with these files you will need SQLite installed on your machine. The data we are using is provided by IMDB at https://datasets.imdbws.com/

## Setup

The first thing we need to do is download and process the data files we need for the practicum. These will be downloaded from IMDB and processed into smaller sample files for ease of working with in the rest of the notebook. In normal circumstances we would do this with a faster language that is better suited for file manipulation in a separate script. For the purposes of this assignment we have included the entire script below as a single chunk. If you are running this yourself and want to either download fresh copies of the data from IMDB, or process existing copies, then you should change the `download` and/or `process` variables to `TRUE` below. But be warned this will take a VERY long time depending on your computer. We have included the processed files with our submission so that you do not have to wait for this unless you want to.

```{r, results='hide', message=FALSE}
library(data.table)
library(hash)
library(readr)
library(tidyr)
library(dplyr)
library(tibble)

# False by default. Change to true if you need these files or want new versions.
# This may necessitate filtering the datasets as well.
# Caution: This will take a long time and may overwhelm your computer!!! 
download <- FALSE
process <- FALSE

# File constants
dataDir <- "./data/"
sampleDataDir <- "./sample-data/"
url <- "https://datasets.imdbws.com/"
nameBasics <- "name.basics"
titleBasics <- "title.basics"
titleAkas <- "title.akas"
titleCrew <- "title.crew"
titleEpisode <- "title.episode"
titlePrincipals <- "title.principals"
titleRatings <- "title.ratings"
gz <- ".tsv.gz"
sampleTsv <- ".sample.tsv"

# ========== Functions ==========

downloadData <- function() {
  #' Downloads all needed data files from IMDB.
  filenames <- c(nameBasics, titleBasics, titleAkas, titleCrew,
                 titleEpisode, titlePrincipals, titleRatings)
  for (name in filenames) {
    website <- paste(url, name, gz, sep="")
    destination <- paste(dataDir, name, gz, sep="")
    download.file(website, destination)
  }
}

createDataframe <- function(filename, rows=-1) {
  #' Creates a dataframe out of a tsv file from IMDB. Can specify a row limit
  #' with rows, but defaults to the whole file.
  fread(filename, na.strings = "\\N", encoding = "UTF-8", data.table = FALSE, 
        showProgress = FALSE, nrows = rows)  # fread is a faster alternative to read.tsv
}

createTitleIdSet <- function(episodeRows, nameRows) {
  #' Creates a set of title ids from the tconst values in the episode and 
  #' name tsv files. These are used due to create a representative dataset
  #' that has both episodes and actors that appear in more than one included
  #' movie (that is not a show).
  epPaths <- getFullAndSamplePaths(titleEpisode)
  namePaths <- getFullAndSamplePaths(nameBasics)
  
  df.eps <- createDataframe(epPaths$full, episodeRows)
  df.names <- createDataframe(namePaths$full, nameRows)
  
  # Separate knownForTitles
  kfTitles <- drop_na(df.names["knownForTitles"]) %>%
    mutate(knownForTitles = strsplit(as.character(knownForTitles), ",")) %>%
    unnest(knownForTitles)
  
  # Combine all titles into a set
  allTitles <- unique(
    union_all(
      as.character(df.eps$tconst),
      as.character(df.eps$parentTconst),
      kfTitles$knownForTitles
    )
  )
  titleSet = hash(allTitles, 1:length(allTitles))  # values don't matter for the set
  
  titleSet
}

createNamesIdSet <- function(princNames, dirNames, writeNames) {
  # Gather all unique name IDs into single series
  allNames <- unique(
    union_all(
      princNames,
      dirNames,
      writeNames
    )
  )
  # Add them to a hash
  nameSet = hash(allNames, 1:length(allNames))  # values don't matter for the set
  
  nameSet
}

writeEpisodeSample <- function(numRows) {
  #' Writes sample data from the episodes tsv. This can be processed faster
  #' than others because we simply select a number of rows to included from
  #' the top.
  paths <- getOldAndNewPaths(titleEpisode)
  df <- createDataframe(paths$full, numRows)
  write_tsv(df, paths$sample)
}

writeFilteredTsv <- function(filename, filterSet, colname = "tconst") {
  #' Writes an existing dataframe after filtering it on tconst
  paths <- getFullAndSamplePaths(filename)
  df <- createDataframe(paths$full)
  
  # This may take several minutes...
  if (colname == "tconst") {
    df <- filter(df, has.key(tconst, filterSet))  
  } else if (colname == "titleId") {
    df <- filter(df, has.key(titleId, filterSet))
  } else if (colname == "nconst") {
    df <- filter(df, has.key(nconst, filterSet)) 
  }
  # Don't filter if colname different
  
  # Write out the data
  write_tsv(df, paths$sample)
  
  # Return the filtered dataframe in case it needs further processing
  df
}

flattenAndWriteCrewToDirectors <- function(crewDataframe) {
  #' Takes the crew data and flattens the directors column so all values are
  #' on their own line and then writes it to disk. Will be used later.
  # Separate directors
  df.directors <- crewDataframe[c("tconst", "directors")] %>%
    mutate(directors = strsplit(as.character(directors), ",")) %>%
    unnest(directors)
  
  # Remove titles with no director listed
  df.directors <- drop_na(df.directors)
  
  # Write
  outfile <- paste(sampleDataDir, "title.directors", sampleTsv, sep="")
  write_tsv(df.directors, outfile)
  
  df.directors
}

flattenAndWriteCrewToWriters <- function(crewDataframe) {
  #' Takes the crew data and flattens the writers column so all values are
  #' on their own line and then writes it to disk. Will be used later.
  # Separate writers
  df.writers <- crewDataframe[c("tconst", "writers")] %>%
    mutate(writers = strsplit(as.character(writers), ",")) %>%
    unnest(writers)
  
  # Remove titles with no director listed
  df.writers <- drop_na(df.writers)
  
  # Write
  outfile <- paste(sampleDataDir, "title.writers", sampleTsv, sep="")
  write_tsv(df.writers, outfile)
  
  df.writers
}

getFullAndSamplePaths <- function(filename) {
  #' Returns the path to the original data file and the sample data file in
  #' a list.
  full <- paste(dataDir, filename, gz, sep="")
  sample <- paste(sampleDataDir, filename, sampleTsv, sep="")
  list("full" = full, "sample" = sample)
}

# ========== Actual processing ==========

if (download) {
  if (!dir.exists(dataDir)) {
    dir.create(dataDir)
  }
  downloadData()
}

if (process) {
  if (!dir.exists(sampleDataDir)) {
    dir.create(sampleDataDir)  
  }
  # Constants for selecting titles to include
  numEpRows <- 500
  numNameRows <- 50
  
  # Get titles to include
  titleIdSet <- createTitleIdSet(numEpRows, numNameRows)
  
  # Process files that we do not need to extract name ids from
  writeEpisodeSample(numEpRows)
  writeFilteredTsv(titleBasics, titleIdSet)
  writeFilteredTsv(titleAkas, titleIdSet, "titleId")
  writeFilteredTsv(titleRatings, titleIdSet)
  
  # Process files that do need to extract name ids
  df <- writeFilteredTsv(titlePrincipals, titleIdSet)
  princ.names <- as.character(df$nconst)
  df <- writeFilteredTsv(titleCrew, titleIdSet)
  dirs.names <- flattenAndWriteCrewToDirectors(df)
  write.names <- flattenAndWriteCrewToWriters(df)

  nameIdSet <- createNamesIdSet(
    princ.names,
    dirs.names$directors,
    write.names$writers
  )

  # Finally, process the names table
  writeFilteredTsv(nameBasics, nameIdSet, "nconst")
}
```

Before working with the data we need to create a place to store it. This section will set up a SQLite database and any constants needed later.

```{r}
library(RSQLite)

# Setup SQLite database
DB_NAME <- "imdb-data.db"
conn <- dbConnect(RSQLite::SQLite(), DB_NAME)
```

## Database Schema

The following chunk creates all the necessary tables in our database based on the schema presented earlier.

```{r, results='hide'}
# First we need to remove any existing data,
# for example, if this has been run before.
drop_table <- function(table_name) {
  paste("DROP TABLE IF EXISTS ", table_name, ";", sep="")
}

# Saving rows affected to a var prevents output
dbExecute(conn, "PRAGMA foreign_keys = OFF;") # Avoid FK checks
curr_tables <- dbListTables(conn)
for (table in curr_tables) {
  dbExecute(conn, drop_table(table))
}
dbExecute(conn, "PRAGMA foreign_keys = ON;")

# Now we can create the tables
build_table <- function(table_def) {
  CREATE <- "CREATE TABLE IF NOT EXISTS"
  paste(CREATE, table_def)
}

# Build table definition list
tables <- c(
  build_table(
    "Formats (
      format_id INTEGER PRIMARY KEY,
      format TEXT NOT NULL
    );"
  ),
  build_table(
    "Media (
      tconst TEXT PRIMARY KEY,
      format_id INTEGER NOT NULL,
      primaryTitle TEXT,
      originalTitle TEXT,
      isAdult INTEGER, -- 0=false, else true
      startYear INTEGER,
      endYear INTEGER,
      runtimeMins INTEGER,
      FOREIGN KEY (format_id) REFERENCES Formats(format_id)
    );"
  ),
  build_table(
    "Ratings (
      tconst TEXT PRIMARY KEY,
      averageRating REAL
      numVotes INTEGER,
      FOREIGN KEY (tconst) REFERENCES Media(tconst)
    );"
  ),
  build_table(
    "Episodes (
      tconst TEXT PRIMARY KEY,
      parentTconst TEXT NOT NULL,
      seasonNumer INTEGER,
      episodeNumber INTEGER,
      FOREIGN KEY (tconst) REFERENCES Media(tconst),
      FOREIGN KEY (parentTconst) REFERENCES Media(tconst)
    );"
  ),
  build_table(
    "Genres (
      genre_id INTEGER PRIMARY KEY,
      genre TEXT NOT NULL
    );"
  ),
  build_table(
    "Media_Genres (
      mg_id INTEGER PRIMARY KEY,
      tconst TEXT NOT NULL,
      genre_id INTEGER NOT NULL,
      FOREIGN KEY (tconst) REFERENCES Media(tconst),
      FOREIGN KEY (genre_id) REFERENCES Genres(genre_id),
      CONSTRAINT unique_mg_tc_gid UNIQUE (tconst, genre_id)
    );"
  ),
  build_table(
    "People (
      nconst TEXT PRIMARY KEY,
      primaryName TEXT,
      birthYear INTEGER,
      deathYear INTEGER,
      age INTEGER,
      numMovies INTEGER
    );"
  ),
  build_table(
    "Known_For_Titles (
      kt_id INTEGER PRIMARY KEY,
      nconst TEXT NOT NULL,
      tconst TEXT NOT NULL,
      FOREIGN KEY (tconst) REFERENCES Media(tconst),
      FOREIGN KEY (nconst) REFERENCES People(nconst),
      CONSTRAINT unique_kft_nc_tc UNIQUE (nconst, tconst)
    );"
  ),
  build_table(
    "Professions (
      prof_id INTEGER PRIMARY KEY,
      prof_title TEXT NOT NULL
    );"
  ),
  build_table(
    "Primary_Profession (
      pp_id INTEGER PRIMARY KEY,
      nconst TEXT NOT NULL,
      prof_id INTEGER NOT NULL,
      FOREIGN KEY (nconst) REFERENCES People(nconst),
      FOREIGN KEY (prof_id) REFERENCES Professions(prof_id),
      CONSTRAINT unique_pp_nc_pid UNIQUE (nconst, prof_id)
    );"
  ),
  build_table(
    "Categories (
      category_id INTEGER PRIMARY KEY,
      category TEXT NOT NULL
    );"
  ),
  build_table(
    "Crew (
      tconst TEXT NOT NULL,
      ordering INTEGER NOT NULL,
      nconst TEXT NOT NULL,
      category_id INTEGER NOT NULL,
      job TEXT,
      characters TEXT,
      PRIMARY KEY (tconst, ordering),
      FOREIGN KEY (tconst) REFERENCES Media(tconst),
      FOREIGN KEY (nconst) REFERENCES People(nconst),
      FOREIGN KEY (category_id) REFERENCES Categories(category_id)
    );"
  ),
  build_table(
    "Characters (
      character_id INTEGER PRIMARY KEY,
      character TEXT NOT NULL
    );"
  ),
  build_table(
    "Cast (
      cast_id INTEGER PRIMARY KEY,
      tconst TEXT NOT NULL,
      ordering INTEGER NOT NULL,
      character_id INTEGER NOT NULL,
      -- Media table PK tconst has ref. integrity through Crew Table
      FOREIGN KEY (tconst, ordering) REFERENCES Crew(tconst, ordering),
      FOREIGN KEY (character_id) REFERENCES Characters(character_id),
      CONSTRAINT unique_crew_tc_ord_tid UNIQUE (tconst, ordering, character_id)
    );"
  ),
  build_table(
    "Media_Directors (
      md_id INTEGER PRIMARY KEY,
      tconst TEXT NOT NULL,
      nconst TEXT NOT NULL,
      FOREIGN KEY (tconst) REFERENCES Media(tconst),
      FOREIGN KEY (nconst) REFERENCES People(nconst),
      CONSTRAINT unique_md_nc_tc UNIQUE (nconst, tconst)
    );"
  ),
  build_table(
    "Media_Writers (
      mw_id INTEGER PRIMARY KEY,
      tconst TEXT NOT NULL,
      nconst TEXT NOT NULL,
      FOREIGN KEY (tconst) REFERENCES Media(tconst),
      FOREIGN KEY (nconst) REFERENCES People(nconst),
      CONSTRAINT unique_mw_nc_tc UNIQUE (nconst, tconst)
    );"
  ),
  build_table(
    "Also_Known_As (
      tconst TEXT NOT NULL,
      ordering INTEGER NOT NULL,
      title TEXT,
      region TEXT,
      language TEXT,
      isOriginalTitle INTEGER, -- 0=false, else true
      PRIMARY KEY (tconst, ordering),
      FOREIGN KEY (tconst) REFERENCES Media(tconst)
    );"
  ),
  build_table(
    "Types (
      type_id INTEGER PRIMARY KEY,
      type TEXT NOT NULL
    );"
  ),
  build_table(
    "Aka_Types (
      akt_id INTEGER PRIMARY KEY,
      tconst TEXT NOT NULL,
      ordering INTEGER NOT NULL,
      type_id INTEGER NOT NULL,
      -- Media table PK tconst has ref. integrity through AKA Table
      FOREIGN KEY (tconst, ordering) REFERENCES Also_Known_As(tconst, ordering),
      FOREIGN KEY (type_id) REFERENCES Types(type_id),
      CONSTRAINT unique_akat_tc_ord_tid UNIQUE (tconst, ordering, type_id)
    );"
  ),
  build_table(
    "Attributes (
      att_id INTEGER PRIMARY KEY,
      att_name TEXT NOT NULL
    );"
  ),
  build_table(
    "Aka_Attributes (
      aka_id INTEGER PRIMARY KEY,
      tconst TEXT NOT NULL,
      ordering INTEGER NOT NULL,
      att_id INTEGER NOT NULL,
      -- Media table PK tconst has ref. integrity through AKA Table
      FOREIGN KEY (tconst, ordering) REFERENCES Also_Known_As(tconst, ordering),
      FOREIGN KEY (att_id) REFERENCES Attributes(att_id),
      CONSTRAINT unique_akaatt_tc_ord_aid UNIQUE (tconst, ordering, att_id)
    );"
  )
)

# Actually create the tables
for (table_stmt in tables) {
  dbExecute(conn, table_stmt)
}
```

## Reading Data Locally

Next we need to gather. Because of the size of the data set, we will take a small sample of it for the purposes of this practicum.

Let's set up some constants first.

```{r}
dir <- "./sample-data/"
ext <- ".sample.tsv"
names <- paste(dir, "name.basics", ext, sep="")
titles <- paste(dir, "title.basics", ext, sep="")
akas <- paste(dir, "title.akas", ext, sep="")
episodes <- paste(dir, "title.episode", ext, sep="")
principals <- paste(dir, "title.principals", ext, sep="")
directors <- paste(dir, "title.directors", ext, sep="")
writers <- paste(dir, "title.writers", ext, sep="")
ratings <- paste(dir, "title.ratings", ext, sep="")
```

### Titles

The number of titles we choose to include will determine the amount of data read in from other files. Therefore we will get this data first.

```{r, message=FALSE}
df.titles <- read_tsv(titles)
head(df.titles)
```

### AKAs

```{r, message=FALSE}
df.akas <- read_tsv(akas)
head(df.akas)
```

### Episodes

```{r, message=FALSE}
df.eps <- read_tsv(episodes)
head(df.eps)
```

### Ratings

```{r, message=FALSE}
df.ratings <- read_tsv(ratings)
head(df.ratings)
```

### Principles

```{r, message=FALSE}
df.princ <- read_tsv(principals)
head(df.princ)
```

### Crew

In pre-processing the crew data was split into seperate sets for directors and writers.

#### Directors

```{r, message=FALSE}
df.dirs <- read_tsv(directors)
head(df.dirs)
```

#### Writers

```{r, message=FALSE}
df.writers <- read_tsv(writers)
head(df.writers)
```

### Names/People

```{r, message=FALSE}
df.names <- read_tsv(names)
head(df.names)
```

## Loading Data to Database

"Formats" table
```{r}
# Grab the needed columns from the current dataframe
types <- df.titles[,c(0,2)]
# Since this is a categorical table, grab unique values
format <- unique(types)
# Create a new dataframe with unique values
df_tt <- data.frame(format)
# Assign unique ids to the unique values - only works on initial loading of data
df_tt <- tibble::rowid_to_column(df_tt, "format_id")
# Transfer the new dataframe into the database table
# Overwrite is true because this should be the initial reading of the data
dbWriteTable(conn,"Formats",df_tt, overwrite=TRUE)
```

"Media" table
```{r}
# Grab the needed columns from the current dataframe
df_media <- df.titles[,-c(0,9)]
# Add a column for the FK id
df_media <- add_column(df_media, format_id = 0, .after = 2)

# Get a list of the categories available
types <- df_tt[,c(0,2)]
head(types)
class(types)
# For each category, assign the FK id
for(term in types) {
  search1 <- df_tt$titleType == term
  # Get the FK id for the category
  id <- df_tt[which(search1),][1,1]
  search2 <- df_media$titleType == term
  # Replace default 0 value with the FK id
  df_media$format_id[search2] <- id
}

# If this was not the initial read of the dataset, we would want to search to ensure there are no default FK ids left.
# If there were, this would mean we would need to add them to the category table and then update the FK ids.
# As the categories were just determined using the same initial dataset, this is not needed at this time.

# Drop the category column, only the FK id column is needed
df_media$titleType <- NULL

# Transfer the new dataframe into the database table
# Overwrite is true because this should be the initial reading of the data
dbWriteTable(conn,"Media",df_media, overwrite=TRUE)
```

"Genres" and "Media_Genres" tables
```{r}
# Grab the needed columns from the current dataframe
df_mgl <- df.titles[,c(1,9)]

# Separate multi-valued attributes
df_mg <- df_mgl %>% 
  mutate(genres = strsplit(genres, ",")) %>%
  unnest(genres)
# Assign unique ids to the unique values - only works on initial loading of data
df_mg <- tibble::rowid_to_column(df_mg, "mg_id")
# Remove titles with no genre listed
df_mg <- drop_na(df_mg)
# Add a column for the FK id
df_mg <- add_column(df_mg, genre_id = 0, .after = 2)
head(df_mg)

# Since genres is a categorical table, grab unique values
df_genres <- unique(df_mg["genres"])
# Assign unique ids to the unique values - only works on initial loading of data
df_genres <- tibble::rowid_to_column(df_genres, "genre_id")
# Rename column names to match table
colnames(df_genres) <- c("genre_id", "genre")
head(df_genres)

# Convert the genre into the appropriate FK id
# Get a vector list of the categories available
types <- df_genres[["genre"]]
head(types)
# For each category, assign the FK id
for(term in types) {
  search1 <- df_genres$genre == term
  # Get the FK id for the category
  id <- df_genres[which(search1),][1,1]
  search2 <- df_mg$genres == term
  # Replace default 0 value with the FK id
  df_mg$genre_id[search2] <- id
}

# If this was not the initial read of the dataset, we would want to search to ensure there are no default FK ids left.
# If there were, this would mean we would need to add them to the category table and then update the FK ids.
# As the categories were just determined using the same initial dataset, this is not needed at this time.

# Drop the category column, only the FK id column is needed
df_mg$genres <- NULL

# Transfer the new dataframes into the database tables
# Overwrite is true because this should be the initial reading of the data
dbWriteTable(conn,"Genres",df_genres, overwrite=TRUE)
dbWriteTable(conn,"Media_Genres",df_mg, overwrite=TRUE)
```

"Episodes" table
```{r}

```

"Ratings" table
```{r}

```

"People" table
```{r}

```

"Known_For_Titles" table
```{r}

```

"Professions" table
```{r}

```

"Primary_Profession" table
```{r}

```

"Categories" table
```{r}

```

"Crew" table
```{r}

```

"Cast" and "Characters" tables
```{r}

```

"Media_Directors" table
```{r}

```

"Media_Writers" table
```{r}

```

"Also_Known_As" table
```{r}

```

"Types" table
```{r}

```

"Aka_Types" table
```{r}

```

"Attributes" table
```{r}

```

"Aka_Attributes" table
```{r}

```
